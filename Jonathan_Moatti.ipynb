{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:34:52.592724Z",
     "start_time": "2020-04-15T22:34:49.674098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmonn\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\mmonn\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy\n",
    "from string import punctuation\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence,plot_objective, plot_evaluations\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import random\n",
    "from torchtext.vocab import GloVe\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Preprocess : from raw Kaggle data to combined  clean DataFrame format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:00.004109Z",
     "start_time": "2020-04-15T22:34:59.960226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25561"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#titre text entrainement\n",
    "data_text = pd.read_csv('Data/text.csv')\n",
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:00.911038Z",
     "start_time": "2020-04-15T22:35:00.894059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12779"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label associe aux titre entrainement\n",
    "data_train_label = pd.read_csv('Data/train.csv')\n",
    "data_train_label.head()\n",
    "len(data_train_label['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:01.636185Z",
     "start_time": "2020-04-15T22:35:01.610243Z"
    }
   },
   "outputs": [],
   "source": [
    "#table de reference\n",
    "data_ref = pd.read_csv('Data/reference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:02.876914Z",
     "start_time": "2020-04-15T22:35:02.859928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   1\n",
       "1   2\n",
       "2   4\n",
       "3   5\n",
       "4   7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('Data/test.csv')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:09.541804Z",
     "start_time": "2020-04-15T22:35:09.534823Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using this functions to associate each text to their IDs, each reference to their text, and each reference to their own text.\n",
    "#Then i concat my titles and their reference, and separate them with /.\n",
    "#Title separated from reference with /\n",
    "#References are split on dot. So . separates two references of the same text.\n",
    "def data_preprocess(x,y,ref):\n",
    "    def ref_func(i,ref_df):\n",
    "        ref_list = list(ref_df[ref_df.id_x==i][\"title\"])\n",
    "        return \". \".join(ref_list)\n",
    "    df_tmp = y.merge(x,left_on=\"id\", right_on=\"id\",how=\"inner\") # Merge the label and text into on df\n",
    "    ref_merge = ref.merge(x,left_on=\"id.1\",right_on=\"id\",how=\"left\")\n",
    "    df_tmp[\"new_var\"] = df_tmp.id.apply(lambda i: ref_func(i,ref_merge))\n",
    "    df_tmp[\"AllCombined\"] = df_tmp[\"title\"] + \" / \" + df_tmp[\"new_var\"]\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:17.755515Z",
     "start_time": "2020-04-15T22:35:11.356614Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_final = data_preprocess(data_text, data_train_label, data_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final.to_csv('data_train_final.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final DataFrame for training set.\n",
    "df = pd.read_csv('Data/data_train_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:35:29.699891Z",
     "start_time": "2020-04-15T22:35:29.687964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>new_var</th>\n",
       "      <th>AllCombined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>interactive visual exploration of neighbor bas...</td>\n",
       "      <td>a framework for clustering evolving data strea...</td>\n",
       "      <td>interactive visual exploration of neighbor bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>relational division four algorithms and their ...</td>\n",
       "      <td>implementation techniques for main memory data...</td>\n",
       "      <td>relational division four algorithms and their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>simplifying xml schema effortless handling of ...</td>\n",
       "      <td>statix making xml count. answering xml queries...</td>\n",
       "      <td>simplifying xml schema effortless handling of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>funbase a function based information managemen...</td>\n",
       "      <td>temporal databases   status and research direc...</td>\n",
       "      <td>funbase a function based information managemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>inverted matrix efficient discovery of frequen...</td>\n",
       "      <td>dynamic itemset counting and implication rules...</td>\n",
       "      <td>inverted matrix efficient discovery of frequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12774</th>\n",
       "      <td>25547</td>\n",
       "      <td>4</td>\n",
       "      <td>scaling up from dialogue to multilogue some pr...</td>\n",
       "      <td>resolving ellipsis in clarification</td>\n",
       "      <td>scaling up from dialogue to multilogue some pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12775</th>\n",
       "      <td>25548</td>\n",
       "      <td>3</td>\n",
       "      <td>a laboratory for the development and evaluatio...</td>\n",
       "      <td></td>\n",
       "      <td>a laboratory for the development and evaluatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12776</th>\n",
       "      <td>25554</td>\n",
       "      <td>2</td>\n",
       "      <td>an analysis of transformational analogy genera...</td>\n",
       "      <td></td>\n",
       "      <td>an analysis of transformational analogy genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12777</th>\n",
       "      <td>25555</td>\n",
       "      <td>2</td>\n",
       "      <td>exploiting known taxonomies in learning overla...</td>\n",
       "      <td>hierarchical classification combining bayes wi...</td>\n",
       "      <td>exploiting known taxonomies in learning overla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12778</th>\n",
       "      <td>25557</td>\n",
       "      <td>1</td>\n",
       "      <td>maintaining materialized views in distributed ...</td>\n",
       "      <td>database snapshots. maintenance of views. the ...</td>\n",
       "      <td>maintaining materialized views in distributed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12779 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              title  \\\n",
       "0          0      1  interactive visual exploration of neighbor bas...   \n",
       "1          3      1  relational division four algorithms and their ...   \n",
       "2          6      1  simplifying xml schema effortless handling of ...   \n",
       "3          8      0  funbase a function based information managemen...   \n",
       "4          9      0  inverted matrix efficient discovery of frequen...   \n",
       "...      ...    ...                                                ...   \n",
       "12774  25547      4  scaling up from dialogue to multilogue some pr...   \n",
       "12775  25548      3  a laboratory for the development and evaluatio...   \n",
       "12776  25554      2  an analysis of transformational analogy genera...   \n",
       "12777  25555      2  exploiting known taxonomies in learning overla...   \n",
       "12778  25557      1  maintaining materialized views in distributed ...   \n",
       "\n",
       "                                                 new_var  \\\n",
       "0      a framework for clustering evolving data strea...   \n",
       "1      implementation techniques for main memory data...   \n",
       "2      statix making xml count. answering xml queries...   \n",
       "3      temporal databases   status and research direc...   \n",
       "4      dynamic itemset counting and implication rules...   \n",
       "...                                                  ...   \n",
       "12774                resolving ellipsis in clarification   \n",
       "12775                                                      \n",
       "12776                                                      \n",
       "12777  hierarchical classification combining bayes wi...   \n",
       "12778  database snapshots. maintenance of views. the ...   \n",
       "\n",
       "                                             AllCombined  \n",
       "0      interactive visual exploration of neighbor bas...  \n",
       "1      relational division four algorithms and their ...  \n",
       "2      simplifying xml schema effortless handling of ...  \n",
       "3      funbase a function based information managemen...  \n",
       "4      inverted matrix efficient discovery of frequen...  \n",
       "...                                                  ...  \n",
       "12774  scaling up from dialogue to multilogue some pr...  \n",
       "12775  a laboratory for the development and evaluatio...  \n",
       "12776  an analysis of transformational analogy genera...  \n",
       "12777  exploiting known taxonomies in learning overla...  \n",
       "12778  maintaining materialized views in distributed ...  \n",
       "\n",
       "[12779 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:36:21.652448Z",
     "start_time": "2020-04-15T22:36:21.627507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ref = pd.read_csv('Data/reference.csv')\n",
    "len(data_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:36:22.719089Z",
     "start_time": "2020-04-15T22:36:22.711112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12782"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('Data/test.csv')\n",
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:36:23.603739Z",
     "start_time": "2020-04-15T22:36:23.570086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25561"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = pd.read_csv('Data/text.csv')\n",
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:36:28.347541Z",
     "start_time": "2020-04-15T22:36:28.340593Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocess(x,y,ref):\n",
    "    def ref_func(i,ref_df):\n",
    "        ref_list = list(ref_df[ref_df.id_x==i][\"title\"])\n",
    "        return \". \".join(ref_list)\n",
    "    df_tmp = y.merge(x,left_on=\"id\", right_on=\"id\",how=\"inner\") # Merge the label and text into on df\n",
    "    ref_merge = ref.merge(x,left_on=\"id.1\",right_on=\"id\",how=\"left\")\n",
    "    df_tmp[\"new_var\"] = df_tmp.id.apply(lambda i: ref_func(i,ref_merge))\n",
    "    df_tmp[\"AllCombined\"] = df_tmp[\"title\"] + \" / \" + df_tmp[\"new_var\"]\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:36:57.255468Z",
     "start_time": "2020-04-15T22:36:51.181121Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_process is my final DataFrame clean and combined on which i will perform my Kaggle Predictions.\n",
    "test_process = data_preprocess(x = data_text, y = data_test, ref = data_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best  Kaggle Model: VotingClassifier with LGBM + MultinomialNB + LinearSVC (Machine Learning).\n",
    "\n",
    "Here i created a VotingClassifier based on three very different learners in order to capture as much feature mapping as possible.\n",
    "> This VotingClassifier gave me my ***best Kaggle performance and is my final model used for the competition***. Scored 83.39% on kaggle board during competition.\n",
    "\n",
    "The base learners used were:\n",
    "\n",
    " 1) LinearSVM (optimized with Bayesian Optimization for strongest performing base learner, ***see code at end of section***). 82.24% validation accuracy, 82.92% kaggle accuracy. Best SINGLE model.\n",
    "\n",
    " 2) MultinomialNB (optimized with Bayesian Optimization for strongest performing base learner, ***see code at end of section***).\n",
    "82.001% validation accuracy. Did not submit this model alone on kaggle (LinearSVM was better)\n",
    "\n",
    " 3) LGBMClassifier (optimized with Bayesian Optimization for strongest performing base learner, ***see code at end of section***).\n",
    "79.39% validation accuracy. Very bad model alone BUT works VERY well in VotingClassifier. Must have interesting feature mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:54:52.945561Z",
     "start_time": "2020-04-15T22:54:52.853838Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/data_train_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:54:53.435327Z",
     "start_time": "2020-04-15T22:54:53.430340Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:02.009127Z",
     "start_time": "2020-04-15T22:54:54.201680Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:02.389969Z",
     "start_time": "2020-04-15T22:55:02.148724Z"
    }
   },
   "outputs": [],
   "source": [
    "# to clean data (taking out special characters and multiple spaces)\n",
    "df['new_AllCombined'] = df['AllCombined'].apply(lambda x: re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \",x))  \n",
    "df['new_AllCombined'] = df['new_AllCombined'].apply(lambda x: re.sub(\"\\s{2,}\", \" \",x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:02.547056Z",
     "start_time": "2020-04-15T22:55:02.544049Z"
    }
   },
   "outputs": [],
   "source": [
    "#df['new_AllCombined'] = df['new_AllCombined'].apply(lambda x: [stemmer.stem(y) for y in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:02.694646Z",
     "start_time": "2020-04-15T22:55:02.691653Z"
    }
   },
   "outputs": [],
   "source": [
    "#stemming or lemmatization both need this.\n",
    "#df['new_AllCombined'] = df['new_AllCombined'].apply(lambda x : ' '.join(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:02.893159Z",
     "start_time": "2020-04-15T22:55:02.837264Z"
    }
   },
   "outputs": [],
   "source": [
    "#take out all non alpha numeric\n",
    "df['new_AllCombined'] = df.new_AllCombined.apply(lambda x: re.sub(r'[^\\w\\s]','', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for LogReg / SVC (to be passed in TfidfVectorizer for preprocessing). STEMMING FUNCT.\n",
    "\n",
    "Certain models got stemming, others lemmatization, others none. I based my decision purely on accuracy. I will not show every single test i made for every single model or this will be too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:03.039730Z",
     "start_time": "2020-04-15T22:55:03.035761Z"
    }
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING FUNCTION FOR VOTINGCLASSIFIER\n",
    "def my_preprocess(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \",text)\n",
    "    text = re.sub(\"\\s{2,}\", \" \",text)\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    # text = text.apply(lambda x: re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \",x)) --> peut pas utiliser de lambda function ici.\n",
    "    # text = text.apply(lambda x: re.sub(\"\\s{2,}\", \" \",x))\n",
    "    # text = text.apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:03.672406Z",
     "start_time": "2020-04-15T22:55:03.182340Z"
    }
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING FUNCTION FOR VOTINGCLASSIFIER\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "def my_tokenizer(doc):\n",
    "    # tokens = doc.split()\n",
    "    tokens = [token.text for token in tokenizer(doc)]\n",
    "    tokens = [stemmer.stem(x) for x in tokens]  # --> l'argument tokenizer de TfidfVectorizer prend une list de tokens, cest pour ca qu'on fait ca ici\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for NB / LGBM (to be passed in TfidfVectorizer for preprocessing). LEMMATIZATION FUNCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:03.825001Z",
     "start_time": "2020-04-15T22:55:03.821010Z"
    }
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING FUNCTION FOR VOTINGCLASSIFIER\n",
    "def my_preprocess1(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \",text)\n",
    "    text = re.sub(\"\\s{2,}\", \" \",text)\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:03.985570Z",
     "start_time": "2020-04-15T22:55:03.981579Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemmatizing moin bon resultats aussi, mais reste du preprocess augmente results\n",
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:04.133411Z",
     "start_time": "2020-04-15T22:55:04.127427Z"
    }
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING FUNCTION FOR VOTINGCLASSIFIER\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "def my_tokenizer1(doc):\n",
    "    def lemmatize_text(tok):\n",
    "        return lemmatizer.lemmatize(tok)\n",
    "    tokens = [token.text for token in tokenizer(doc)]\n",
    "    tokens = [lemmatize_text(x) for x in tokens]  # --> l'argument tokenizer de TfidfVectorizer prend une list de tokens, cest pour ca qu'on fait ca ici\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:04.288003Z",
     "start_time": "2020-04-15T22:55:04.276078Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['new_AllCombined'], df['label'], stratify = df['label'], shuffle = True, random_state = 42, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:04.440844Z",
     "start_time": "2020-04-15T22:55:04.431880Z"
    }
   },
   "outputs": [],
   "source": [
    "#best model pipeline/params with 82.24% acc solo\n",
    "#LinearSVC + stemming\n",
    "text_clf_svc = Pipeline([('tfidf', TfidfVectorizer(ngram_range = (1,6),\n",
    "                                                stop_words = None,\n",
    "                                                preprocessor = my_preprocess,\n",
    "                                                tokenizer = my_tokenizer)),\n",
    "                        \n",
    "                         ('clf', LinearSVC(C=1.0069937972047027,\n",
    "                                           loss = 'hinge',\n",
    "                                           tol = 0.5,\n",
    "                                           max_iter = 100000000, \n",
    "                                           fit_intercept=False, \n",
    "                                           random_state=42))])\n",
    "\n",
    "#pour train\n",
    "# text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:04.598425Z",
     "start_time": "2020-04-15T22:55:04.592439Z"
    }
   },
   "outputs": [],
   "source": [
    "#82% solo\n",
    "#MultinomialNB + lemmatization\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,6), \n",
    "                                               stop_words=STOP_WORDS, \n",
    "                                               norm = 'l1', \n",
    "                                               strip_accents=None,\n",
    "                                               lowercase = True,\n",
    "                                               analyzer = 'word',\n",
    "                                               preprocessor = my_preprocess1,\n",
    "                                               tokenizer = my_tokenizer1)),\n",
    "                     \n",
    "                    ('nb_model', MultinomialNB(alpha = 0.01852770799933675,\n",
    "                                           fit_prior = False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:05.104490Z",
     "start_time": "2020-04-15T22:55:05.101499Z"
    }
   },
   "outputs": [],
   "source": [
    "#text_clf_lr = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,4), \n",
    "#                                                  stop_words='english',\n",
    "#                                                  preprocessor = my_preprocess,\n",
    "#                                                  tokenizer = my_tokenizer)), \n",
    "#                        \n",
    "#                        ('lr', LogisticRegression(C=4.5,multi_class='ovr',random_state=42, max_iter=100000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:05.678935Z",
     "start_time": "2020-04-15T22:55:05.674947Z"
    }
   },
   "outputs": [],
   "source": [
    "##skopt model. Version Lemmatization + tree.\n",
    "#text_clf_lgb = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,1),\n",
    "#                                  stop_words = None,\n",
    "#                                  norm = 'l2',\n",
    "#                                  tokenizer = my_tokenizer1, \n",
    "#                                  preprocessor = my_preprocess1)),\n",
    "#                         \n",
    "#                    ('lgb', LGBMClassifier(importance_type = 'split',\n",
    "#                                           boosting_type = 'gbdt',\n",
    "#                                           learning_rate = 0.044216356170560425,\n",
    "#                                           n_estimators = 820,\n",
    "#                                           subsample = 0.6380506568792133,\n",
    "#                                           colsample_bytree = 0.9154582072806454,\n",
    "#                                           reg_alpha = 0.2045409147444816,\n",
    "#                                           reg_lambda = 0.7049387326724629,\n",
    "#                                           random_state = 42,\n",
    "#                                           n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:06.289627Z",
     "start_time": "2020-04-15T22:55:06.284609Z"
    }
   },
   "outputs": [],
   "source": [
    "#skopt model. Version Lemmatization + dart. (best model mix) - 79.37% solo\n",
    "#LGBMClassifier + lemmatization.\n",
    "text_clf_lgb = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,1),\n",
    "                                  stop_words = None,\n",
    "                                  norm = 'l2',\n",
    "                                  tokenizer = my_tokenizer1, \n",
    "                                  preprocessor = my_preprocess1)),\n",
    "                         \n",
    "                    ('lgb', LGBMClassifier(importance_type = 'split',\n",
    "                                           boosting_type = 'dart',\n",
    "                                           learning_rate = 0.058067079891947884,\n",
    "                                           n_estimators = 1903,\n",
    "                                           subsample = 0.6941823248699718,\n",
    "                                           colsample_bytree = 0.6649289756607163,\n",
    "                                           reg_alpha = 0.3617243544568278,\n",
    "                                           reg_lambda = 1.354455342583696,\n",
    "                                           random_state = 42,\n",
    "                                           n_jobs = -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:06.850214Z",
     "start_time": "2020-04-15T22:55:06.847190Z"
    }
   },
   "outputs": [],
   "source": [
    "##benchmark (avec stopwords et stemming)\n",
    "#text_clf_lgb = Pipeline([('tfidf', TfidfVectorizer(stop_words = STOP_WORDS,\n",
    "#                                                   tokenizer=my_tokenizer, \n",
    "#                                                   preprocessor=my_preprocess)),\n",
    "#                         \n",
    "#                         ('lgb',LGBMClassifier(random_state = 42, \n",
    "#                                               max_iter = 10000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:07.393671Z",
     "start_time": "2020-04-15T22:55:07.389714Z"
    }
   },
   "outputs": [],
   "source": [
    "#RandomForstClassifier\n",
    "#text_clf_rf = Pipeline([('tfidf', TfidfVectorizer(ngram_range = (1,2),\n",
    "#                                                   tokenizer=my_tokenizer, \n",
    "#                                                   preprocessor=my_preprocess)),\n",
    "#                         \n",
    "#                         ('lgb',RandomForestClassifier(random_state = 42, \n",
    "#                                               ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:08.370035Z",
     "start_time": "2020-04-15T22:55:08.365041Z"
    }
   },
   "outputs": [],
   "source": [
    "#BEST VOTING CLASSIFIER. ALSO BEST KAGGLE MODEL. 82.76% valid accuracy, 83.39% Kaggle Accuracy.\n",
    "#82.76% avec new tweaked lgb + stemming + dart\n",
    "voting_clf8 = VotingClassifier(\n",
    "    estimators=[('nb', text_clf_nb), ('lgb', text_clf_lgb),('svc', text_clf_svc)],\n",
    "    voting = 'hard',\n",
    "    weights = [1,1,1]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:09.413043Z",
     "start_time": "2020-04-15T22:55:09.410051Z"
    }
   },
   "outputs": [],
   "source": [
    "#82.13%\n",
    "#voting_clf1 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('svc', text_clf_svc)],\n",
    "#    voting = 'hard'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:09.675341Z",
     "start_time": "2020-04-15T22:55:09.671351Z"
    }
   },
   "outputs": [],
   "source": [
    "#82.42%\n",
    "#voting_clf2 = VotingClassifier(\n",
    "#    estimators=[('nb', text_clf_nb), ('svc', text_clf_svc)],\n",
    "#    voting = 'hard'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:09.916695Z",
     "start_time": "2020-04-15T22:55:09.912706Z"
    }
   },
   "outputs": [],
   "source": [
    "#81,71%\n",
    "#voting_clf3 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb)],\n",
    "#    voting = 'soft'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:10.437642Z",
     "start_time": "2020-04-15T22:55:10.434650Z"
    }
   },
   "outputs": [],
   "source": [
    "# 81,11%\n",
    "#voting_clf4 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('lgb', text_clf_lgb)],\n",
    "#    voting = 'soft'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:11.026048Z",
     "start_time": "2020-04-15T22:55:11.023056Z"
    }
   },
   "outputs": [],
   "source": [
    "# 81,89%%\n",
    "#voting_clf5 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('lgb', text_clf_lgb)],\n",
    "#    voting = 'hard'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:11.573472Z",
     "start_time": "2020-04-15T22:55:11.569483Z"
    }
   },
   "outputs": [],
   "source": [
    "#82.47%\n",
    "#voting_clf6 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('lgb', text_clf_lgb),('svc', text_clf_svc)],\n",
    "#    voting = 'hard',\n",
    "#    \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:11.812865Z",
     "start_time": "2020-04-15T22:55:11.809840Z"
    }
   },
   "outputs": [],
   "source": [
    "#82.42\n",
    "#voting_clf7 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('lgb', text_clf_lgb),('svc', text_clf_svc)],\n",
    "#    voting = 'hard',\n",
    "#    weights = [1,1.1,0.9,1.2]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:12.044250Z",
     "start_time": "2020-04-15T22:55:12.041228Z"
    }
   },
   "outputs": [],
   "source": [
    "#82.23%\n",
    "#voting_clf9 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('lgb', text_clf_lgb),('svc', text_clf_svc),('rf', text_clf_rf)],\n",
    "#    voting = 'hard',\n",
    "#    \n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T22:55:12.716541Z",
     "start_time": "2020-04-15T22:55:12.713550Z"
    }
   },
   "outputs": [],
   "source": [
    "#80.28%\n",
    "#voting_clf10 = VotingClassifier(\n",
    "#    estimators=[('lr',text_clf_lr), ('nb', text_clf_nb), ('lgb', text_clf_lgb),('rf', text_clf_rf)],\n",
    "#    voting = 'soft'\n",
    "#\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:01:01.649798Z",
     "start_time": "2020-04-15T22:56:09.340659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmonn\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', 'd', 'doe', 'ha', 'le', 'll', 'm', 'n', 'nt', 's', 't', 'u', 've', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb',\n",
       "                              Pipeline(memory=None,\n",
       "                                       steps=[('tfidf',\n",
       "                                               TfidfVectorizer(analyzer='word',\n",
       "                                                               binary=False,\n",
       "                                                               decode_error='strict',\n",
       "                                                               dtype=<class 'numpy.float64'>,\n",
       "                                                               encoding='utf-8',\n",
       "                                                               input='content',\n",
       "                                                               lowercase=True,\n",
       "                                                               max_df=1.0,\n",
       "                                                               max_features=None,\n",
       "                                                               min_df=1,\n",
       "                                                               ngram_range=(1,\n",
       "                                                                            6),\n",
       "                                                               norm='l1',\n",
       "                                                               preprocessor=<function my_preprocess1 at 0x000002011DDDBE58>,\n",
       "                                                               smooth_idf...\n",
       "                                                               tokenizer=<function my_tokenizer at 0x00000201202E54C8>,\n",
       "                                                               use_idf=True,\n",
       "                                                               vocabulary=None)),\n",
       "                                              ('clf',\n",
       "                                               LinearSVC(C=1.0069937972047027,\n",
       "                                                         class_weight=None,\n",
       "                                                         dual=True,\n",
       "                                                         fit_intercept=False,\n",
       "                                                         intercept_scaling=1,\n",
       "                                                         loss='hinge',\n",
       "                                                         max_iter=100000000,\n",
       "                                                         multi_class='ovr',\n",
       "                                                         penalty='l2',\n",
       "                                                         random_state=42,\n",
       "                                                         tol=0.5, verbose=0))],\n",
       "                                       verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=[1, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR Training (X_train, y_train)\n",
    "voting_clf8.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:34:34.683326Z",
     "start_time": "2020-04-15T06:28:18.428977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmonn\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', 'd', 'doe', 'ha', 'le', 'll', 'm', 'n', 'nt', 's', 't', 'u', 've', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('nb',\n",
       "                              Pipeline(memory=None,\n",
       "                                       steps=[('tfidf',\n",
       "                                               TfidfVectorizer(analyzer='word',\n",
       "                                                               binary=False,\n",
       "                                                               decode_error='strict',\n",
       "                                                               dtype=<class 'numpy.float64'>,\n",
       "                                                               encoding='utf-8',\n",
       "                                                               input='content',\n",
       "                                                               lowercase=True,\n",
       "                                                               max_df=1.0,\n",
       "                                                               max_features=None,\n",
       "                                                               min_df=1,\n",
       "                                                               ngram_range=(1,\n",
       "                                                                            6),\n",
       "                                                               norm='l1',\n",
       "                                                               preprocessor=<function my_preprocess1 at 0x000002CC28DE9558>,\n",
       "                                                               smooth_idf...\n",
       "                                                               tokenizer=<function my_tokenizer at 0x000002CC28DC5948>,\n",
       "                                                               use_idf=True,\n",
       "                                                               vocabulary=None)),\n",
       "                                              ('clf',\n",
       "                                               LinearSVC(C=1.0069937972047027,\n",
       "                                                         class_weight=None,\n",
       "                                                         dual=True,\n",
       "                                                         fit_intercept=False,\n",
       "                                                         intercept_scaling=1,\n",
       "                                                         loss='hinge',\n",
       "                                                         max_iter=100000000,\n",
       "                                                         multi_class='ovr',\n",
       "                                                         penalty='l2',\n",
       "                                                         random_state=42,\n",
       "                                                         tol=0.5, verbose=0))],\n",
       "                                       verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=[1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR Final prediction. Fitting on WHOLE dataset instead of (X_train, y_train)\n",
    "voting_clf8.fit(df['new_AllCombined'], df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:01:30.875470Z",
     "start_time": "2020-04-15T23:01:24.969796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275952008346374"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best model results at 82.759%\n",
    "voting_clf8.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:03.217072Z",
     "start_time": "2020-04-15T06:38:03.174160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25561"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = pd.read_csv('Data/text.csv')\n",
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:04.165383Z",
     "start_time": "2020-04-15T06:38:04.149397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12782"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('Data/test.csv')\n",
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:05.499695Z",
     "start_time": "2020-04-15T06:38:05.469775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73313"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ref = pd.read_csv('Data/reference.csv')\n",
    "len(data_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:08.112037Z",
     "start_time": "2020-04-15T06:38:08.105057Z"
    }
   },
   "outputs": [],
   "source": [
    "#utilisation fonction pour faire le merging avec references\n",
    "def data_preprocess(x,y,ref):\n",
    "    def ref_func(i,ref_df):\n",
    "        ref_list = list(ref_df[ref_df.id_x==i][\"title\"])\n",
    "        return \". \".join(ref_list)\n",
    "    df_tmp = y.merge(x,left_on=\"id\", right_on=\"id\",how=\"inner\") # Merge the label and text into on df\n",
    "    ref_merge = ref.merge(x,left_on=\"id.1\",right_on=\"id\",how=\"left\")\n",
    "    df_tmp[\"new_var\"] = df_tmp.id.apply(lambda i: ref_func(i,ref_merge))\n",
    "    df_tmp[\"AllCombined\"] = df_tmp[\"title\"] + \" / \" + df_tmp[\"new_var\"]\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:15.541509Z",
     "start_time": "2020-04-15T06:38:09.047296Z"
    }
   },
   "outputs": [],
   "source": [
    "test_process = data_preprocess(x = data_text, y = data_test, ref = data_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:19.468546Z",
     "start_time": "2020-04-15T06:38:19.448600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>new_var</th>\n",
       "      <th>AllCombined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>autodomainmine a graphical data mining system ...</td>\n",
       "      <td>what is the nearest neighbor in high dimension...</td>\n",
       "      <td>autodomainmine a graphical data mining system ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>anipqo almost non intrusive parametric query o...</td>\n",
       "      <td>parametric query optimization. optimization of...</td>\n",
       "      <td>anipqo almost non intrusive parametric query o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>selection and ranking of text from highly impe...</td>\n",
       "      <td></td>\n",
       "      <td>selection and ranking of text from highly impe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>conditional random fields for multi agent rein...</td>\n",
       "      <td>sequential optimality and coordination in mult...</td>\n",
       "      <td>conditional random fields for multi agent rein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>multi dimensional description logics</td>\n",
       "      <td></td>\n",
       "      <td>multi dimensional description logics /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12777</th>\n",
       "      <td>25553</td>\n",
       "      <td>currency based updates to distributed material...</td>\n",
       "      <td>database snapshots. maintenance of views. impl...</td>\n",
       "      <td>currency based updates to distributed material...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12778</th>\n",
       "      <td>25556</td>\n",
       "      <td>dynamic typing in a statically typed language</td>\n",
       "      <td>quasi static typing. an ideal model for recurs...</td>\n",
       "      <td>dynamic typing in a statically typed language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12779</th>\n",
       "      <td>25558</td>\n",
       "      <td>learning sparse metrics via linear programming</td>\n",
       "      <td>fastmap a fast algorithm for indexing data min...</td>\n",
       "      <td>learning sparse metrics via linear programming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12780</th>\n",
       "      <td>25559</td>\n",
       "      <td>computer assisted reasoning with mizar</td>\n",
       "      <td></td>\n",
       "      <td>computer assisted reasoning with mizar /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12781</th>\n",
       "      <td>25560</td>\n",
       "      <td>characterization of a large web site populatio...</td>\n",
       "      <td>gigascope a stream database for network applic...</td>\n",
       "      <td>characterization of a large web site populatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12782 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          1  autodomainmine a graphical data mining system ...   \n",
       "1          2  anipqo almost non intrusive parametric query o...   \n",
       "2          4  selection and ranking of text from highly impe...   \n",
       "3          5  conditional random fields for multi agent rein...   \n",
       "4          7               multi dimensional description logics   \n",
       "...      ...                                                ...   \n",
       "12777  25553  currency based updates to distributed material...   \n",
       "12778  25556      dynamic typing in a statically typed language   \n",
       "12779  25558     learning sparse metrics via linear programming   \n",
       "12780  25559             computer assisted reasoning with mizar   \n",
       "12781  25560  characterization of a large web site populatio...   \n",
       "\n",
       "                                                 new_var  \\\n",
       "0      what is the nearest neighbor in high dimension...   \n",
       "1      parametric query optimization. optimization of...   \n",
       "2                                                          \n",
       "3      sequential optimality and coordination in mult...   \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "12777  database snapshots. maintenance of views. impl...   \n",
       "12778  quasi static typing. an ideal model for recurs...   \n",
       "12779  fastmap a fast algorithm for indexing data min...   \n",
       "12780                                                      \n",
       "12781  gigascope a stream database for network applic...   \n",
       "\n",
       "                                             AllCombined  \n",
       "0      autodomainmine a graphical data mining system ...  \n",
       "1      anipqo almost non intrusive parametric query o...  \n",
       "2      selection and ranking of text from highly impe...  \n",
       "3      conditional random fields for multi agent rein...  \n",
       "4                multi dimensional description logics /   \n",
       "...                                                  ...  \n",
       "12777  currency based updates to distributed material...  \n",
       "12778  dynamic typing in a statically typed language ...  \n",
       "12779  learning sparse metrics via linear programming...  \n",
       "12780          computer assisted reasoning with mizar /   \n",
       "12781  characterization of a large web site populatio...  \n",
       "\n",
       "[12782 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:30.608145Z",
     "start_time": "2020-04-15T06:38:30.343658Z"
    }
   },
   "outputs": [],
   "source": [
    "# to clean data (taking out special characters and multiple spaces)\n",
    "test_process['new_AllCombined'] = test_process['AllCombined'].apply(lambda x: re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \",x))  \n",
    "test_process['new_AllCombined'] = test_process['new_AllCombined'].apply(lambda x: re.sub(\"\\s{2,}\", \" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:38:36.098198Z",
     "start_time": "2020-04-15T06:38:36.036368Z"
    }
   },
   "outputs": [],
   "source": [
    "#take out all non alpha numeric\n",
    "test_process['new_AllCombined'] = test_process.new_AllCombined.apply(lambda x: re.sub(r'[^\\w\\s]','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:40:02.540477Z",
     "start_time": "2020-04-15T06:39:39.622628Z"
    }
   },
   "outputs": [],
   "source": [
    "#This is my final prediction to kaggle. (voting_clf8 was fitted on the WHOLE dataset before final preds.)\n",
    "y_pred = voting_clf8.predict(test_process.new_AllCombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:40:26.658781Z",
     "start_time": "2020-04-15T06:40:26.654792Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:40:32.850475Z",
     "start_time": "2020-04-15T06:40:32.842469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      2\n",
       "4      2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:40:45.416669Z",
     "start_time": "2020-04-15T06:40:45.402709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12777</th>\n",
       "      <td>25553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12778</th>\n",
       "      <td>25556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12779</th>\n",
       "      <td>25558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12780</th>\n",
       "      <td>25559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12781</th>\n",
       "      <td>25560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12782 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id\n",
       "0          1\n",
       "1          2\n",
       "2          4\n",
       "3          5\n",
       "4          7\n",
       "...      ...\n",
       "12777  25553\n",
       "12778  25556\n",
       "12779  25558\n",
       "12780  25559\n",
       "12781  25560\n",
       "\n",
       "[12782 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_id_df = pd.DataFrame(test_process.id, columns = ['id'])\n",
    "y_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:40:56.934561Z",
     "start_time": "2020-04-15T06:40:56.929574Z"
    }
   },
   "outputs": [],
   "source": [
    "final = pd.concat([y_id_df, y_pred_df], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:40:58.025435Z",
     "start_time": "2020-04-15T06:40:58.017455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1      0\n",
       "1   2      1\n",
       "2   4      0\n",
       "3   5      2\n",
       "4   7      2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:41:17.841242Z",
     "start_time": "2020-04-15T06:41:17.815296Z"
    }
   },
   "outputs": [],
   "source": [
    "#BEST PREDICTIONS FOR KAGGLE IN THIS CSV. 83.39%.\n",
    "final.to_csv('VOTING_FINAL.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:41:33.625734Z",
     "start_time": "2020-04-15T06:41:33.609779Z"
    }
   },
   "outputs": [],
   "source": [
    "fin = pd.read_csv('VOTING_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T06:41:37.349009Z",
     "start_time": "2020-04-15T06:41:37.341995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1      0\n",
       "1   2      1\n",
       "2   4      0\n",
       "3   5      2\n",
       "4   7      2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for each Base Learner.\n",
    "\n",
    "Each base learning was trained for minimum 500 iterations. (LGBMClassifier 500 iteration, MultinomialNB 1500 iterations, LinearSVC 2500 iterations). What you see plugged in is the warmstart of my final best hyper-parameters for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian LGBM : LGBMClassifier SKopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(func):\n",
    "    def wrapper(*args,**kwargs):\n",
    "        res = func(*args,**kwargs)\n",
    "        print(*args,**kwargs)\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters \n",
    "\n",
    "importance_type = Categorical(('split','gain'), name = 'importance_type')\n",
    "#boosting_type = Categorical(('gbdt', 'dart'), name = 'boosting_type')\n",
    "learning_rate = Real(low = 0.001, high = 0.2001, name = 'learning_rate')\n",
    "#max_depth = Integer(low = 1, high = 15, name = 'max_depth')\n",
    "n_estimators = Integer(low = 20, high = 3000, name = 'n_estimators')\n",
    "subsample = Real(low = 0.5, high = 1, name = 'subsample')\n",
    "colsample_bytree = Real(low = 0.5, high = 1, name = 'colsample_bytree')\n",
    "reg_alpha = Real(low = 0, high = 15, name = 'reg_alpha')\n",
    "reg_lambda = Real(low = 0, high = 15, name = 'reg_lambda')\n",
    "#ngram_range_sup = Integer(low=1, high=3, name='ngram_range_sup')\n",
    "norm = Categorical(('l1','l2'), name = 'norm')\n",
    "#stop_words = Categorical((None, 'english'), name = 'stop_words')\n",
    "\n",
    "dimensions = [importance_type, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda, norm]\n",
    "\n",
    "nb_calls = 350\n",
    "\n",
    "default_parameters = ['split', 0.058067079891947884, 1903, 0.6941823248699718, 0.6649289756607163, 0.3617243544568278, 1.354455342583696, 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@result\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(importance_type, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda, norm):\n",
    "    \n",
    "    text_clf = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(#ngram_range=(1,ngram_range_sup),\n",
    "                                  #stop_words = stop_words,\n",
    "                                  norm = norm,\n",
    "                                  tokenizer = my_tokenizer1, \n",
    "                                  preprocessor = my_preprocess1\n",
    "                                  )),\n",
    "                         \n",
    "            ('lgb', LGBMClassifier(importance_type = importance_type,\n",
    "                                   boosting_type = 'dart',\n",
    "                                   learning_rate = learning_rate,\n",
    "                                   #max_depth = max_depth,\n",
    "                                   n_estimators = n_estimators,\n",
    "                                   subsample = subsample,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   reg_alpha = reg_alpha,\n",
    "                                   reg_lambda = reg_lambda,\n",
    "                                   random_state = 42,\n",
    "                                   n_jobs = -1\n",
    "                                  ))])\n",
    "    \n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    accurracy = text_clf.score(X_test, y_test)\n",
    "    return -accurracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',\n",
    "                                n_calls=nb_calls,\n",
    "                                x0=default_parameters,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=True,\n",
    "                                random_state=4\n",
    "                               )\n",
    "    \n",
    "print(search_result.x)\n",
    "print(search_result.fun)\n",
    "plot = plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baysian LinearSVM : LinearSVM Skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(func):\n",
    "    def wrapper(*args,**kwargs):\n",
    "        res = func(*args,**kwargs)\n",
    "        print(*args,**kwargs)\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters \n",
    "\n",
    "# multi_class = Categorical(('auto', 'ovr','multinomial'), name = 'multi_class')\n",
    "#dual = Categorical((True, False), name = 'dual')\n",
    "C = Real(low=0.01, high=90, name= 'C')\n",
    "ngram_range_sup = Integer(low=2, high=10, name='ngram_range_sup')\n",
    "loss = Categorical(('hinge', 'squared_hinge'), name = 'loss')\n",
    "stop_words = Categorical(('english', None), name = 'stop_words')\n",
    "norm = Categorical(('l1','l2'), name = 'norm')\n",
    "#fit_intercept = Categorical((True,False), name = 'fit_intercept')\n",
    "tol = Real(low=0.00001, high = 25, name = 'tol')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = [C, ngram_range_sup, loss, stop_words, norm, tol]\n",
    "\n",
    "\n",
    "nb_calls = 500\n",
    "\n",
    "default_parameters = [1.0069937972047027, 6, 'hinge', None, 'l2', 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@result\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(C, ngram_range_sup, loss, stop_words, norm, tol):\n",
    "    text_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,ngram_range_sup), \n",
    "                                                   norm = norm, \n",
    "                                                   stop_words = stop_words,\n",
    "                                                   tokenizer = my_tokenizer,\n",
    "                                                   preprocessor = my_preprocess)),\n",
    "                         \n",
    "                         \n",
    "                          ('clf', \n",
    "                           LinearSVC(C=C, \n",
    "                                     max_iter=10000, \n",
    "                                     loss=loss,  \n",
    "                                     fit_intercept=False, \n",
    "                                     tol = tol, \n",
    "                                     random_state=42))])\n",
    "    \n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    accurracy = text_clf.score(X_test, y_test)\n",
    "    return -accurracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',\n",
    "                                n_calls=nb_calls,\n",
    "                                x0=default_parameters,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=True,\n",
    "                                random_state=4,\n",
    "                                xi=0.001\n",
    "                               )\n",
    "    \n",
    "print(search_result.x)\n",
    "print(search_result.fun)\n",
    "plot = plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian NB : MultinomialNB Skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:41:23.252604Z",
     "start_time": "2020-04-15T20:41:23.244625Z"
    }
   },
   "outputs": [],
   "source": [
    "def result(func):\n",
    "    def wrapper(*args,**kwargs):\n",
    "        res = func(*args,**kwargs)\n",
    "        print(*args,**kwargs)\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters \n",
    "\n",
    "alpha = Real(low=0.00001, high=12, name= 'alpha')\n",
    "ngram_range_sup = Integer(low=2, high=22, name='ngram_range_sup')\n",
    "fit_prior = Categorical((False, True), name = 'fit_prior')\n",
    "stop_words = Categorical(('english', None), name = 'stop_words')\n",
    "norm = Categorical(('l1', 'l2'), name = 'norm')\n",
    "strip_accents = Categorical((None, 'unicode'), name = 'strip_accents')\n",
    "lowercase = Categorical((True, False), name = 'lowercase')\n",
    "analyzer = Categorical(('word', 'char'), name = 'analyzer')\n",
    "#essayer avec Tokenizer = 'spacy' aussi, mais marche juste si analyzer = 'word'\n",
    "\n",
    "\n",
    "dimensions = [alpha, ngram_range_sup, fit_prior, stop_words, norm, strip_accents, lowercase, analyzer]\n",
    "\n",
    "\n",
    "nb_calls = 800\n",
    "\n",
    "#warmstart with best params to date. 81.95% best accuracy with MultiNB et lemmatization (clean, lemmatize, join, last cleaning function)\n",
    "default_parameters = [0.01852770799933675, 6, False, 'english', 'l1', None, True, 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@result\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(alpha, ngram_range_sup, fit_prior, stop_words, norm, strip_accents, lowercase, analyzer):\n",
    "    text_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,ngram_range_sup),\n",
    "                                                   stop_words = stop_words, \n",
    "                                                   norm=norm,\n",
    "                                                   strip_accents = strip_accents, \n",
    "                                                   lowercase=lowercase, \n",
    "                                                   analyzer=analyzer,\n",
    "                                                   tokenizer = my_tokenizer1,\n",
    "                                                   preprocessor = my_preprocess1)),\n",
    "                         \n",
    "                    ('clf', MultinomialNB(alpha=alpha, \n",
    "                                          fit_prior=fit_prior))])\n",
    "    \n",
    "    text_clf.fit(X_train, y_train)\n",
    "    accurracy = text_clf.score(X_test, y_test)\n",
    "    return -accurracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',\n",
    "                                n_calls=nb_calls,\n",
    "                                x0=default_parameters,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=True,\n",
    "                                random_state=4\n",
    "                               )\n",
    "    \n",
    "print(search_result.x)\n",
    "print(search_result.fun)\n",
    "plot = plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END OF PRESENTATION OF MY BEST KAGGLE MODEL. END OF MACHINE LEARNING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Deep Learning Part.\n",
    "\n",
    "Best Deep Learning model:\n",
    "\n",
    "> CNN with FastText embedding.\n",
    "\n",
    "Deep Learning Models tested:\n",
    "\n",
    "1) Vanilla RNN (60% validation accuracy, not tested on kaggle submission)\n",
    "\n",
    "2) Vanilla LSTM (72% validation accuracy, not tested on kaggle submission)\n",
    "\n",
    "3) Bi-directional, multi-layer LSTM (with GloVe 840B embedding) (81.1% validation accuracy, 81% test kaggle accuracy)\n",
    "\n",
    "4) Bi-directional, multi-layer LSTM with BERT (81.5% validation accuracy, 81% test kaggle accuracy)\n",
    "\n",
    "5) CNN with GloVe embedding + Conv1D and Conv2D choice models. (83% validation accuracy, 81.6% test kaggle accuracy, i think i have either overfitting problem or a problem with my prediction function with this model. In theory it should be my best model of all, but i have problems when submitting to kaggle.)\n",
    "\n",
    "> Note: I will not post my Vanilla RNN and Vanilla LSTM as they were used to build my best LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best LSTM : Bi-directional, GloVe 840B, Multi-Layer, double dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:04:58.726213Z",
     "start_time": "2020-04-15T23:04:58.713244Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:00.084976Z",
     "start_time": "2020-04-15T23:04:59.595548Z"
    }
   },
   "outputs": [],
   "source": [
    "#include_lengths = True because we are using packed padded sequences\n",
    "TEXT = data.Field(sequential = True, \n",
    "                  lower = True, \n",
    "                  use_vocab = True,\n",
    "                  tokenize = 'spacy', \n",
    "                  include_lengths = True, \n",
    "                  batch_first=True)\n",
    "\n",
    "LABEL = data.LabelField(sequential = False, use_vocab = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:00.688280Z",
     "start_time": "2020-04-15T23:05:00.684255Z"
    }
   },
   "outputs": [],
   "source": [
    "#defining tuples (le premier du tuple doit match notre nom de dataset tabulaire, le deuxieme doit match son field)\n",
    "fields = [(\"id\",None),(\"label\",LABEL),(\"title\",None),(\"new_var\",None),(\"AllCombined\",TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:03.188722Z",
     "start_time": "2020-04-15T23:05:01.812804Z"
    }
   },
   "outputs": [],
   "source": [
    "#importer les donnees dependement de si on a train, valid, test ou bien juste train / test. Ici on a juste un fichier train, voir sentiment 1 pour multi fichier options (pas de .split apres TabularDataset)\n",
    "train = data.TabularDataset(path = 'Data/data_train_final.csv',\n",
    "                                format = 'csv',\n",
    "                                fields = fields,\n",
    "                                skip_header = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:03.360695Z",
     "start_time": "2020-04-15T23:05:03.341744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10223 2556\n"
     ]
    }
   ],
   "source": [
    "#split en train et valid\n",
    "train_data, valid_data = train.split(split_ratio = 0.8, random_state = random.seed(SEED))\n",
    "print(f'{(len(train_data))} {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:07.953035Z",
     "start_time": "2020-04-15T23:05:03.995700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9971"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, \n",
    "                 vectors = \"glove.840B.300d\", \n",
    "                 #glove.840B.300d\n",
    "                 #glove.6B.100d\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "# Find the padding index and the vocab size\n",
    "padding_idx = TEXT.vocab.stoi[\"<pad>\"]\n",
    "vocab_size = len(TEXT.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:09.679074Z",
     "start_time": "2020-04-15T23:05:08.843888Z"
    }
   },
   "outputs": [],
   "source": [
    "#create iterators after defining batch size and device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#creation de l'iterateur\n",
    "(train_iter, valid_iter) = data.BucketIterator.splits((train_data, valid_data), \n",
    "                                                    batch_sizes = (32,32), #batch_size pour un argument, batch_sizes pour tuple\n",
    "                                                    device = device,\n",
    "                                                    sort_key = lambda x: len(x.AllCombined),\n",
    "                                                    sort_within_batch = True\n",
    "                                                   #sort_within_batch sert pour le padding\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:10.517021Z",
     "start_time": "2020-04-15T23:05:10.508074Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #rajoute padding_idx = pad_idx pour dire au model comment skip le padding.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        #defini le lstm\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers = n_layers,\n",
    "                           bidirectional = bidirectional,\n",
    "                           dropout = 0 if n_layers < 2 else dropout,\n",
    "                           batch_first = True)\n",
    "        \n",
    "        #defini le linear (le * 2 viens du fait qu'on va utiliser bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        #defini le dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    #As we are passing the lengths of our sentences to be able to use packed padded sequences, \n",
    "    #we have to add a second argument, `text_lengths`, to `forward`.    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        #comme on a pas fait batch_first = True, notre batch size est pas en premier dans le tensor\n",
    "        \n",
    "        #pack sequence:\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first = True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        #unpack sequence:\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "         #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:11.473537Z",
     "start_time": "2020-04-15T23:05:11.405695Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 250\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 4\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTM(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:12.498005Z",
     "start_time": "2020-04-15T23:05:12.493042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,609,805 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:13.351665Z",
     "start_time": "2020-04-15T23:05:13.346698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9971, 300])\n"
     ]
    }
   ],
   "source": [
    "#copying pre-trained word embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:14.437041Z",
     "start_time": "2020-04-15T23:05:14.427069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1032, -1.6268,  0.5729,  ...,  0.3180, -0.1626, -0.0417],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        ...,\n",
       "        [ 0.0495, -0.2737, -0.2819,  ..., -0.2686,  0.5445,  0.1999],\n",
       "        [ 0.3195,  0.2435, -0.1777,  ..., -0.0243, -0.6011,  0.2368],\n",
       "        [-0.0907, -0.1088,  1.1903,  ..., -1.1861,  1.3736, -0.4054]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace initial weights\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:15.233971Z",
     "start_time": "2020-04-15T23:05:15.225962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
      "        ...,\n",
      "        [ 0.0495, -0.2737, -0.2819,  ..., -0.2686,  0.5445,  0.1999],\n",
      "        [ 0.3195,  0.2435, -0.1777,  ..., -0.0243, -0.6011,  0.2368],\n",
      "        [-0.0907, -0.1088,  1.1903,  ..., -1.1861,  1.3736, -0.4054]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:16.018459Z",
     "start_time": "2020-04-15T23:05:16.012477Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:17.480553Z",
     "start_time": "2020-04-15T23:05:17.463786Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:22.518251Z",
     "start_time": "2020-04-15T23:05:18.286206Z"
    }
   },
   "outputs": [],
   "source": [
    "#model we instanciated earlier, pass it to GPU.\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:22.671539Z",
     "start_time": "2020-04-15T23:05:22.664567Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainning loop (modified version of homework1 trainning func)\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.AllCombined # IMPORTANT : ligne a ajouter quand on PACK. Utiliser notre field de text dans batch. On fait ca a cause du packing\n",
    "        logits = model(text.cuda(), text_lengths) #comme on a unpack au dessus, no motre batch. comme dans le rnn\n",
    "        \n",
    "        predictions = torch.max(logits, dim=-1)[1]\n",
    "        loss = criterion(logits, batch.label.cuda()) #ici a la place de label faut mettre le nom de notre y du tabular dataset initial ('label' pour nous)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "        total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator), total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:22.831245Z",
     "start_time": "2020-04-15T23:05:22.824281Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation loop (modified version of homework1 eval func)\n",
    "def evaluate(model, iterator, criterion):  \n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.AllCombined #IMPORTANT, ajouter cette ligne quand PACK.utiliser notre field de text dans batch. On fait ca a cause du packing\n",
    "            logits = model(text.cuda(), text_lengths) #comme on a unpack au dessus, no motre batch. comme dans le rnn\n",
    "            predictions = torch.max(logits, dim=-1)[1]\n",
    "            loss = criterion(logits, batch.label.cuda())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "            total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator), total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:22.984294Z",
     "start_time": "2020-04-15T23:05:22.979307Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:05:23.137883Z",
     "start_time": "2020-04-15T23:05:23.133894Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-15T23:05:58.313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validations Accuracy so far: 0.000 at Epoch 1\n",
      "\n",
      "Epoch 1     | Epoch Time: 0m 19s \n",
      "        | Train loss    0.848| Train acc    0.783 | Valid loss    0.609 | Valid acc    0.795 | + \n",
      "Epoch 2     | Epoch Time: 0m 18s \n",
      "        | Train loss    0.538| Train acc    0.828 | Valid loss    0.589 | Valid acc    0.800 | + \n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "best_epoch = 1\n",
    "\n",
    "# model 2 100 x 300, 4 layers, bidirectional --> 81.2\n",
    "PATH = f\"LSTMrmsp1000.pt\"\n",
    "if os.path.exists(PATH):\n",
    "    print(\"Loading model from last checkpoint...\")\n",
    "    state = torch.load(PATH)\n",
    "    model.load_state_dict(state['best_state_dict'])\n",
    "    best_valid_acc = state['best_valid_acc']\n",
    "    best_epoch = state['epoch']\n",
    "    has_checkpoint = True\n",
    "print(f\"Best Validations Accuracy so far: {best_valid_acc:.3f} at Epoch {best_epoch}\\n\")\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    real_epoch = best_epoch + epoch\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion) #rentrer train_iter et valid_iter qu'on a defini dans notre BucketIterator lors du preprocess\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        print(f'Epoch {real_epoch: <{5}} | Epoch Time: {epoch_mins}m {epoch_secs}s \\n\\\n",
    "        | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} | + ')\n",
    "        #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        \n",
    "        if not os.path.exists('Models'):\n",
    "            os.makedirs('Models')\n",
    "                \n",
    "            # Let's create the checkpoint data to save\n",
    "        checkpoint = {\n",
    "            'epoch': real_epoch,\n",
    "            'best_valid_acc': best_valid_acc,\n",
    "            'best_state_dict': best_state_dict,\n",
    "        }\n",
    "        torch.save(checkpoint, PATH)\n",
    "    else:\n",
    "        print(f'Epoch {real_epoch: <{5}} | Epoch Time: {epoch_mins}m {epoch_secs}s \\n\\\n",
    "              |  Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=f\"LSTMrmsp1000.pt\"\n",
    "state = torch.load(PATH)\n",
    "model.load_state_dict(state['best_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred pour la soumission une fois qu'on a trouve le meilleur model\n",
    "#voir detail dans Programming Pytorch for Deep Learning\n",
    "def prediction(test):\n",
    "    model.eval()\n",
    "    processed = TEXT.process([TEXT.preprocess(test)])\n",
    "    text, text_lengths = processed\n",
    "    preds = model(text.cuda(), text_lengths).argmax().item()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [[test_process.id[i], prediction(test_process.AllCombined[i])] for i in range(len(test_process))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns = ['id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.to_csv('y_pred_df_rmsp1.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best CNN Models with GloVe and choice between Conv2D model or Conv1D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:06.755414Z",
     "start_time": "2020-04-15T23:15:05.303073Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "#torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', batch_first = True)\n",
    "LABEL = data.LabelField(use_vocab = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:07.300618Z",
     "start_time": "2020-04-15T23:15:07.296630Z"
    }
   },
   "outputs": [],
   "source": [
    "#defining tuples (le premier du tuple doit match notre nom de dataset tabulaire, le deuxieme doit match son field)\n",
    "fields = [(\"id\",None),(\"label\",LABEL),(\"title\",None),(\"new_var\",None),(\"AllCombined\",TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:09.030776Z",
     "start_time": "2020-04-15T23:15:07.936503Z"
    }
   },
   "outputs": [],
   "source": [
    "#importer les donnees dependement de si on a train, valid, test ou bien juste train / test. Ici on a juste un fichier train, voir sentiment 1 pour multi fichier options (pas de .split apres TabularDataset)\n",
    "train = data.TabularDataset(path = 'Data/data_train_final.csv',\n",
    "                                format = 'csv',\n",
    "                                fields = fields,\n",
    "                                skip_header = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:09.113519Z",
     "start_time": "2020-04-15T23:15:09.095565Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train.split(split_ratio = 0.8, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:10.095407Z",
     "start_time": "2020-04-15T23:15:10.080588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has 10223 observations \n",
      "Validation has 2556 observations\n"
     ]
    }
   ],
   "source": [
    "print(f'Training has {len(train_data)} observations \\nValidation has {len(valid_data)} observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:13.973980Z",
     "start_time": "2020-04-15T23:15:11.524768Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, \n",
    "                 vectors = \"glove.840B.300d\", \n",
    "                 unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:14.005130Z",
     "start_time": "2020-04-15T23:15:13.995158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9971"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:14.728875Z",
     "start_time": "2020-04-15T23:15:14.482872Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                    batch_sizes = (50,50),\n",
    "                                                    device = device,\n",
    "                                                    sort_key = lambda x: x.AllCombined,\n",
    "                                                    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:18.072853Z",
     "start_time": "2020-04-15T23:15:18.063860Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "            \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1,\n",
    "                                              out_channels = n_filters,\n",
    "                                              kernel_size = (fs, embedding_dim))\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent_len, batch_size]\n",
    "        \n",
    "        #text = text.permute(1,0)\n",
    "        \n",
    "        #text = [batch_size, sent_len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch_size, sent_len, emb_dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:19.135930Z",
     "start_time": "2020-04-15T23:15:19.103023Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 5\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:19.599702Z",
     "start_time": "2020-04-15T23:15:19.594721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,353,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:20.277689Z",
     "start_time": "2020-04-15T23:15:20.267714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8215,  1.1231, -0.1510,  ..., -0.4182, -0.9061, -0.3446],\n",
       "        [ 1.0604,  0.2917,  0.2861,  ...,  1.1272, -0.0987,  1.1603],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        ...,\n",
       "        [ 0.0495, -0.2737, -0.2819,  ..., -0.2686,  0.5445,  0.1999],\n",
       "        [ 0.3195,  0.2435, -0.1777,  ..., -0.0243, -0.6011,  0.2368],\n",
       "        [-0.5658,  0.8013, -0.1055,  ...,  1.6674,  0.0561, -0.2571]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:21.108315Z",
     "start_time": "2020-04-15T23:15:21.091540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        ...,\n",
       "        [ 0.0495, -0.2737, -0.2819,  ..., -0.2686,  0.5445,  0.1999],\n",
       "        [ 0.3195,  0.2435, -0.1777,  ..., -0.0243, -0.6011,  0.2368],\n",
       "        [-0.5658,  0.8013, -0.1055,  ...,  1.6674,  0.0561, -0.2571]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:25.419518Z",
     "start_time": "2020-04-15T23:15:23.398546Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:25.453045Z",
     "start_time": "2020-04-15T23:15:25.444073Z"
    }
   },
   "outputs": [],
   "source": [
    "# train a model\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch.AllCombined.cuda()) #a la place de .text (dans batch.text) il \n",
    "                                            #faut mettre le X du tabular dataset initial ('title' pour nous)\n",
    "        \n",
    "        predictions = torch.max(logits, dim=-1)[1]\n",
    "        loss = criterion(logits, batch.label.cuda()) #ici a la place de label faut mettre le nom de notre y du tabular dataset initial ('label' pour nous)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "        total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator), total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:25.483974Z",
     "start_time": "2020-04-15T23:15:25.477978Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate(model, iterator, criterion):  \n",
    "    total_loss, total_correct, total_prediction = 0.0, 0.0, 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            logits = model(batch.AllCombined.cuda()) #changer text pour title dans batch.text\n",
    "            predictions = torch.max(logits, dim=-1)[1]\n",
    "            loss = criterion(logits, batch.label.cuda())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += torch.eq(predictions, batch.label.cuda()).sum().item()\n",
    "            total_prediction += batch.label.size(0)\n",
    "    return total_loss / len(iterator), total_correct / total_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:25.514898Z",
     "start_time": "2020-04-15T23:15:25.509936Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:15:25.775595Z",
     "start_time": "2020-04-15T23:15:25.771608Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T23:16:06.840921Z",
     "start_time": "2020-04-15T23:15:31.715179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validations Accuracy so far: 0.000 at Epoch 1\n",
      "\n",
      "Epoch 1     | Epoch Time: 0m 8s \n",
      "        | Train loss    1.366| Train acc    0.477 | Valid loss    1.092 | Valid acc    0.710 | + \n",
      "Epoch 2     | Epoch Time: 0m 7s \n",
      "        | Train loss    0.956| Train acc    0.701 | Valid loss    0.805 | Valid acc    0.751 | + \n",
      "Epoch 3     | Epoch Time: 0m 7s \n",
      "        | Train loss    0.762| Train acc    0.749 | Valid loss    0.704 | Valid acc    0.761 | + \n",
      "Epoch 4     | Epoch Time: 0m 7s \n",
      "        | Train loss    0.676| Train acc    0.771 | Valid loss    0.658 | Valid acc    0.773 | + \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a779b0a3183e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#rentrer train_iter et valid_iter qu'on a defini dans notre BucketIterator lors du preprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-82190fd1b2e4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ici a la place de label faut mettre le nom de notre y du tabular dataset initial ('label' pour nous)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "best_epoch = 1\n",
    "\n",
    "# model 2 100 x 300, 2 layers, unidirectional --> 80.9\n",
    "PATH = f\"cnn12-test.pt\"\n",
    "if os.path.exists(PATH):\n",
    "    print(\"Loading model from last checkpoint...\")\n",
    "    state = torch.load(PATH)\n",
    "    model.load_state_dict(state['best_state_dict'])\n",
    "    best_valid_acc = state['best_valid_acc']\n",
    "    best_epoch = state['epoch']\n",
    "    has_checkpoint = True\n",
    "print(f\"Best Validations Accuracy so far: {best_valid_acc:.3f} at Epoch {best_epoch}\\n\")\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    real_epoch = best_epoch + epoch\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion) #rentrer train_iter et valid_iter qu'on a defini dans notre BucketIterator lors du preprocess\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        print(f'Epoch {real_epoch: <{5}} | Epoch Time: {epoch_mins}m {epoch_secs}s \\n\\\n",
    "        | Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} | + ')\n",
    "        #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        \n",
    "        if not os.path.exists('Models'):\n",
    "            os.makedirs('Models')\n",
    "                \n",
    "            # Let's create the checkpoint data to save\n",
    "        checkpoint = {\n",
    "            'epoch': real_epoch,\n",
    "            'best_valid_acc': best_valid_acc,\n",
    "            'best_state_dict': best_state_dict,\n",
    "        }\n",
    "        torch.save(checkpoint, PATH)\n",
    "    else:\n",
    "        print(f'Epoch {real_epoch: <{5}} | Epoch Time: {epoch_mins}m {epoch_secs}s \\n\\\n",
    "              |  Train loss {train_loss:8.3f}| Train acc {train_acc:8.3f} | Valid loss {valid_loss:8.3f} | Valid acc {valid_acc:8.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can follow all the same steps and just change Conv2D model for Conv1D model. Dosnt change the accuracy much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(model, field, text):\n",
    "    model.eval()\n",
    "    text = field.preprocess(text)\n",
    "    text = field.process([text])\n",
    "    x = torch.tensor(text)\n",
    "    # x = autograd.Variable(x)\n",
    "    x = x.cuda()\n",
    "    logits = model(x)\n",
    "    y_pred = torch.max(logits, dim=-1)[1]\n",
    "    y_pred = y_pred.item()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_preds = [[test_preprocessed.id[i],predict_cnn(CNN, TEXTcnn, test_preprocessed.AllCombined[i])] for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_cnn = pd.DataFrame(data=cnn_preds,columns=['id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_cnn.to_csv(\"kaggle_cnn_best.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
